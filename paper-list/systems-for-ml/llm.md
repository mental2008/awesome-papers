# Large Language Model (LLM)

## Training

|   Name   | Conference                                             | Institution                                                                                    | Links                                                                                                                                                                                                                  | Remarks                                                 |
| :------: | ------------------------------------------------------ | ---------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------- |
| **Alpa** | [OSDI 2022](../../reading-notes/conference/osdi-2022/) | <ul><li>UC Berkeley</li><li>AWS</li><li>Google</li><li>SJTU</li><li>CMU</li><li>Duke</li></ul> | <ul><li><a href="https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin">Paper</a></li><li><a href="https://github.com/alpa-projects/alpa">Code</a></li><li><a href="https://alpa.ai/">Docs</a></li></ul> | Generalize the search through _parallelism strategies_. |

## Inference

<table><thead><tr><th width="139">Name</th><th width="132" align="center">Conference</th><th width="143">Institution</th><th width="147">Links</th><th>Remarks</th></tr></thead><tbody><tr><td><strong>Deja Vu</strong></td><td align="center"><a href="../../reading-notes/conference/icml-2023.md">ICML 2023</a></td><td><ul><li>Rice</li><li>ZJU</li><li>Stanford</li><li>UCSD</li><li>ETH</li><li>Adobe</li><li>Meta AI</li><li>CMU</li></ul></td><td><ul><li><a href="https://proceedings.mlr.press/v202/liu23am.html">Paper</a></li><li><a href="https://github.com/FMInference/DejaVu">Code</a></li></ul></td><td>A system to predict <em>contextual sparsity</em> (small, input-dependent sets that yield <em>approximately</em> the same output).</td></tr><tr><td><strong>FlexGen</strong></td><td align="center"><a href="../../reading-notes/conference/icml-2023.md">ICML 2023</a></td><td><ul><li>Stanford</li><li>UC Berkeley</li><li>ETH</li><li>Yandex</li><li>HSE University</li><li>Meta</li><li>CMU</li></ul></td><td><ul><li><a href="../../reading-notes/miscellaneous/arxiv/2023/high-throughput-generative-inference-of-large-language-models-with-a-single-gpu.md">Personal Notes</a></li><li><a href="https://arxiv.org/abs/2303.06865">Paper</a></li><li><a href="https://github.com/FMInference/FlexGen">Code</a></li></ul></td><td><em>High-throughput serving; only use a single GPU.</em></td></tr><tr><td><strong>FastServe</strong></td><td align="center">arXiv 2305.05920</td><td><ul><li>PKU</li></ul></td><td><ul><li><a href="https://arxiv.org/abs/2305.05920">Paper</a></li></ul></td><td>Skip-join multi-level feedback queue scheduling instead of first-come-frist-serve; proactive kv cache swapping; compared to <strong>Orca</strong>.</td></tr><tr><td><strong>AlpaServe</strong></td><td align="center"><a href="../../reading-notes/conference/osdi-2023.md">OSDI 2023</a></td><td><ul><li>UC Berkeley</li><li>PKU</li><li>UPenn</li><li>Stanford</li><li>Google</li></ul></td><td><ul><li><a href="https://arxiv.org/abs/2302.11665">Paper</a></li></ul></td><td>Trade-off between <em>the overhead of model parallelism</em> and <em>reduced serving latency by statistical multiplexing</em>.</td></tr><tr><td><strong>PaLM Inference</strong></td><td align="center"><a href="../../reading-notes/conference/mlsys-2023.md">MLSys 2023</a></td><td><ul><li>Google</li></ul></td><td><ul><li><a href="https://arxiv.org/abs/2211.05102">Paper</a></li><li><a href="https://github.com/google-research/google-research/tree/master/scaling_transformer_inference_efficiency">Code</a></li></ul></td><td>Model partitioning; PaLM; TPUv4.</td></tr><tr><td><strong>DeepSpeed-Inference</strong></td><td align="center"><a href="../../reading-notes/conference/sc-2022.md">SC 2022</a></td><td><ul><li>Microsoft</li></ul></td><td><ul><li><a href="https://dl.acm.org/doi/abs/10.5555/3571885.3571946">Paper</a></li><li><a href="https://github.com/microsoft/DeepSpeed">Code</a></li><li><a href="https://www.deepspeed.ai/inference/">Homepage</a></li></ul></td><td>Leverage <em>CPU/NVMe/GPU memory</em>.</td></tr><tr><td><strong>Orca</strong></td><td align="center"><a href="../../reading-notes/conference/osdi-2022/">OSDI 2022</a></td><td><ul><li>Seoul National University</li><li>FriendliAI</li></ul></td><td><ul><li><a href="../../reading-notes/conference/osdi-2022/orca.md">Personal Notes</a></li><li><a href="https://www.usenix.org/conference/osdi22/presentation/yu">Paper</a></li></ul></td><td><em>Iteration-level</em> scheduling; <em>selective batching</em>.</td></tr></tbody></table>

## Models

|    Model    | Conference       | Institution                       | Links                                                                                                                                                                                                              | Remarks                                                                             |
| :---------: | ---------------- | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------- |
| **LLaMA 2** | arXiv 2307.09288 | <ul><li>Meta AI</li></ul>         | <ul><li><a href="https://arxiv.org/abs/2307.09288">Paper</a></li><li><a href="https://ai.meta.com/llama/">Homepage</a></li></ul>                                                                                   | Released with a _permissive_ community license and is available for commercial use. |
|  **LLaMA**  | arXiv 2302.13971 | <ul><li>Meta AI</li></ul>         | <ul><li><a href="https://arxiv.org/abs/2302.13971">Paper</a></li><li><a href="https://github.com/facebookresearch/llama">Code</a></li></ul>                                                                        | \[**6.7B, 13B, 32.5B, 65.2B**]; _open-access_.                                      |
|  **BLOOM**  | arXiv 2211.05100 | <ul><li>Multiple groups</li></ul> | <ul><li><a href="https://arxiv.org/abs/2211.05100">Paper</a></li><li><a href="https://huggingface.co/bigscience/bloom">Model</a></li><li><a href="https://bigscience.huggingface.co/blog/bloom">Blog</a></li></ul> | **176B**; _open-access_.                                                            |
|   **PaLM**  | arXiv 2204.02311 | <ul><li>Google Research</li></ul> | <ul><li><a href="https://arxiv.org/abs/2204.02311">Paper</a></li><li><a href="https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html">PaLM API</a></li></ul>                            | **540B**; open access to PaLM APIs in March 2023.                                   |
