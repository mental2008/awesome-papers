# Large Language Model (LLM)

## Training

|   Name   | Conference                                             | Institution                                                                                    | Links                                                                                                                                                                                                                  | Remarks                                                 |
| :------: | ------------------------------------------------------ | ---------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------- |
| **Alpa** | [OSDI 2022](../../reading-notes/conference/osdi-2022/) | <ul><li>UC Berkeley</li><li>AWS</li><li>Google</li><li>SJTU</li><li>CMU</li><li>Duke</li></ul> | <ul><li><a href="https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin">Paper</a></li><li><a href="https://github.com/alpa-projects/alpa">Code</a></li><li><a href="https://alpa.ai/">Docs</a></li></ul> | Generalize the search through _parallelism strategies_. |

## Inference

|           Name          | Conference                                                 | Institution                                                                                                               | Links                                                                                                                                                                                                                                                                                                         | Remarks                                                                                                                               |
| :---------------------: | ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
|      **FastServe**      | arXiv 2305.05920                                           | <ul><li>PKU</li></ul>                                                                                                     | <ul><li><a href="https://arxiv.org/abs/2305.05920">Paper</a></li></ul>                                                                                                                                                                                                                                        | Skip-join multi-level feedback queue scheduling instead of first-come-frist-serve; proactive kv cache swapping; compared to **Orca**. |
|      **AlpaServe**      | [OSDI 2023](../../reading-notes/conference/osdi-2023.md)   | <ul><li>UC Berkeley</li><li>PKU</li><li>UPenn</li><li>Stanford</li><li>Google</li></ul>                                   | <ul><li><a href="https://arxiv.org/abs/2302.11665">Paper</a></li></ul>                                                                                                                                                                                                                                        | Trade-off between _the overhead of model parallelism_ and _reduced serving latency by statistical multiplexing_.                      |
|       **FlexGen**       | ICML 2023                                                  | <ul><li>Stanford</li><li>UC Berkeley</li><li>ETH</li><li>Yandex</li><li>HSE University</li><li>Meta</li><li>CMU</li></ul> | <ul><li><a href="../../reading-notes/miscellaneous/arxiv/2023/high-throughput-generative-inference-of-large-language-models-with-a-single-gpu.md">Personal Notes</a></li><li><a href="https://arxiv.org/abs/2303.06865">Paper</a></li><li><a href="https://github.com/FMInference/FlexGen">Code</a></li></ul> | _High-throughput serving; only use a single GPU._                                                                                     |
|    **PaLM Inference**   | [MLSys 2023](../../reading-notes/conference/mlsys-2023.md) | <ul><li>Google</li></ul>                                                                                                  | <ul><li><a href="https://arxiv.org/abs/2211.05102">Paper</a></li><li><a href="https://github.com/google-research/google-research/tree/master/scaling_transformer_inference_efficiency">Code</a></li></ul>                                                                                                     | Model partitioning; PaLM; TPUv4.                                                                                                      |
| **DeepSpeed-Inference** | [SC 2022](../../reading-notes/conference/sc-2022.md)       | <ul><li>Microsoft</li></ul>                                                                                               | <ul><li><a href="https://dl.acm.org/doi/abs/10.5555/3571885.3571946">Paper</a></li><li><a href="https://github.com/microsoft/DeepSpeed">Code</a></li><li><a href="https://www.deepspeed.ai/inference/">Homepage</a></li></ul>                                                                                 | Leverage _CPU/NVMe/GPU memory_.                                                                                                       |
|         **Orca**        | [OSDI 2022](../../reading-notes/conference/osdi-2022/)     | <ul><li>Seoul National University</li><li>FriendliAI</li></ul>                                                            | <ul><li><a href="../../reading-notes/conference/osdi-2022/orca-a-distributed-serving-system-for-transformer-based-generative-models.md">Personal Notes</a></li><li><a href="https://www.usenix.org/conference/osdi22/presentation/yu">Paper</a></li></ul>                                                     | Iteration-level scheduling; selective batching.                                                                                       |

## Models

|    Model    | Conference       | Institution                       | Links                                                                                                                                                                                                              | Remarks                                           |
| :---------: | ---------------- | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------- |
| **LLaMA 2** | arXiv 2307.09288 | <ul><li>Meta AI</li></ul>         | <ul><li><a href="https://arxiv.org/abs/2307.09288">Paper</a></li><li><a href="https://ai.meta.com/llama/">Homepage</a></li></ul>                                                                                   |                                                   |
|  **LLaMA**  | arXiv 2302.13971 | <ul><li>Meta AI</li></ul>         | <ul><li><a href="https://arxiv.org/abs/2302.13971">Paper</a></li><li><a href="https://github.com/facebookresearch/llama">Code</a></li></ul>                                                                        | \[**6.7B, 13B, 32.5B, 65.2B**]; _open-access_.    |
|  **BLOOM**  | arXiv 2211.05100 | Multiple groups                   | <ul><li><a href="https://arxiv.org/abs/2211.05100">Paper</a></li><li><a href="https://huggingface.co/bigscience/bloom">Model</a></li><li><a href="https://bigscience.huggingface.co/blog/bloom">Blog</a></li></ul> | **176B**; _open-access_.                          |
|   **PaLM**  | arXiv 2204.02311 | <ul><li>Google Research</li></ul> | <ul><li><a href="https://arxiv.org/abs/2204.02311">Paper</a></li><li><a href="https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html">PaLM API</a></li></ul>                            | **540B**; open access to PaLM APIs in March 2023. |
