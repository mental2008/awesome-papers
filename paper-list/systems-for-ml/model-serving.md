# Model Serving

## Inference Serving System

|           Name          | Conference                                             | Institution                                                                                                               | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Remarks                                                                                                             |
| :---------------------: | ------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- |
|       **FlexGen**       | arXiv 2303.06865                                       | <ul><li>Stanford</li><li>UC Berkeley</li><li>ETH</li><li>Yandex</li><li>HSE University</li><li>Meta</li><li>CMU</li></ul> | <ul><li><a href="../../reading-notes/miscellaneous/arxiv/2023/high-throughput-generative-inference-of-large-language-models-with-a-single-gpu.md">Personal Notes</a></li><li><a href="https://arxiv.org/abs/2303.06865">Paper</a></li><li><a href="https://github.com/FMInference/FlexGen">Code</a></li></ul>                                                                                                                                                                           | _High-throughput large language model (LLM) serving; single GPU._                                                   |
| **DeepSpeed-Inference** | [SC 2022](../../reading-notes/conference/sc-2022.md)   | <ul><li>Microsoft</li></ul>                                                                                               | <ul><li><a href="https://dl.acm.org/doi/abs/10.5555/3571885.3571946">Paper</a></li><li><a href="https://github.com/microsoft/DeepSpeed">Code</a></li><li><a href="https://www.deepspeed.ai/inference/">Homepage</a></li></ul>                                                                                                                                                                                                                                                           | _Minimize inference latency_ while _maximizing throughput_; **transformer models**; leverage _CPU/NVMe/GPU memory_. |
|         **Reef**        | [OSDI 2022](../../reading-notes/conference/osdi-2022/) | <ul><li>SJTU</li></ul>                                                                                                    | <ul><li><a href="../../reading-notes/conference/osdi-2022/microsecond-scale-preemption-for-concurrent-gpu-accelerated-dnn-inferences.md">Personal Notes</a></li><li><a href="https://www.usenix.org/conference/osdi22/presentation/han">Paper</a></li><li><a href="https://github.com/SJTU-IPADS/reef">Code</a></li><li><a href="https://github.com/SJTU-IPADS/disb">Benchmark</a></li><li><a href="https://github.com/SJTU-IPADS/reef-artifacts/tree/osdi22-ae">Artifact</a></li></ul> | GPU _kernel preemption_; _concurrent_ DNN inferences.                                                               |
|        **INFaaS**       | [ATC 2021](../../Conference/ATC-2021/)                 | <ul><li>Stanford</li></ul>                                                                                                | <ul><li><a href="https://www.usenix.org/conference/atc21/presentation/romero">Paper</a></li><li><a href="https://github.com/stanford-mast/INFaaS">Code</a></li></ul>                                                                                                                                                                                                                                                                                                                    | Consider model-variants.                                                                                            |
|       **Clipper**       | [NSDI 2017](../../Conference/NSDI-2017/)               | <ul><li>UC Berkeley</li></ul>                                                                                             | <ul><li><a href="../../Conference/NSDI-2017/clipper.md">Personal Notes</a></li><li><a href="https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/crankshaw">Paper</a></li><li><a href="https://github.com/ucbrise/clipper">Code</a></li></ul>                                                                                                                                                                                                                       | Caching, batching, adaptive model selection.                                                                        |
|  **TensorFlow Serving** | NIPS 2017 Workshop on ML Systems                       | <ul><li>Google</li></ul>                                                                                                  | <ul><li><a href="https://arxiv.org/abs/1712.06139">Paper</a></li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                     |

## Auto-Configuration System

|      Name     | Conference                                               | Institution                                                          | Links                                                                                                                                                                                                                                                                                                                                  | Remarks                                       |
| :-----------: | -------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------- |
|   **Falcon**  | [SoCC 2022](../../reading-notes/conference/socc-2022/)   | <ul><li>Institute of Software, Chinese Academy of Sciences</li></ul> | <ul><li><a href="../../reading-notes/conference/socc-2022/serving-unseen-deep-learning-model-with-near-optimal-configurations-a-fast-adaptive-search-approach.md">Personal Notes</a></li><li><a href="https://dl.acm.org/doi/10.1145/3542929.3563485">Paper</a></li><li><a href="https://github.com/dos-lab/Falcon">Code</a></li></ul> | Characterize a DL model by its key operators. |
| **Morphling** | [SoCC 2021](../../reading-notes/conference/socc-2021.md) | <ul><li>HKUST</li><li>Alibaba</li></ul>                              | <ul><li><a href="https://dl.acm.org/doi/10.1145/3472883.3486987">Paper</a></li><li><a href="https://github.com/kubedl-io/morphling">Code</a></li></ul>                                                                                                                                                                                 | Meta learning; bayesian optimization.         |

## Survey

| Survey                                                                                          |                     Conference                    | Institute                                                                                                                   | Links                                                                  |
| ----------------------------------------------------------------------------------------------- | :-----------------------------------------------: | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| A survey of multi-tenant deep learning inference on GPU                                         | MLSys 2022 Workshop on Cloud Intelligence / AIOps | <ul><li>George Mason University</li><li>Microsoft</li><li>University of Maryland</li></ul>                                  | <ul><li><a href="https://arxiv.org/abs/2203.09040">arXiv</a></li></ul> |
| A survey of large-scale deep learning serving system optimization: Challenges and opportunities |                  arXiv 2111.14247                 | <ul><li>George Mason University</li><li>Microsoft</li><li>University of Pittsburgh</li><li>University of Maryland</li></ul> | <ul><li><a href="https://arxiv.org/abs/2111.14247">arXiv</a></li></ul> |
