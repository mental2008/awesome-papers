# Model Serving

## Inference Serving System

Large language models (LLMs) are _hot_ and _diverse_ compared to conventional models. Therefore, I have classified the related works for LLMs in [another paper list](llm.md).

|          Name          | Conference                                             | Institution                   | Links                                                                                                                                                                                                                                                                                                                                                                                                             | Remarks                                               |
| :--------------------: | ------------------------------------------------------ | ----------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |
|        **REEF**        | [OSDI 2022](../../reading-notes/conference/osdi-2022/) | <ul><li>SJTU</li></ul>        | <ul><li><a href="../../reading-notes/conference/osdi-2022/reef.md">Personal Notes</a></li><li><a href="https://www.usenix.org/conference/osdi22/presentation/han">Paper</a></li><li><a href="https://github.com/SJTU-IPADS/reef">Code</a></li><li><a href="https://github.com/SJTU-IPADS/disb">Benchmark</a></li><li><a href="https://github.com/SJTU-IPADS/reef-artifacts/tree/osdi22-ae">Artifact</a></li></ul> | GPU _kernel preemption_; _concurrent_ DNN inferences. |
|       **INFaaS**       | [ATC 2021](../../Conference/ATC-2021/)                 | <ul><li>Stanford</li></ul>    | <ul><li><a href="https://www.usenix.org/conference/atc21/presentation/romero">Paper</a></li><li><a href="https://github.com/stanford-mast/INFaaS">Code</a></li></ul>                                                                                                                                                                                                                                              | **Best Paper**; consider _model-variants_.            |
|       **Clipper**      | [NSDI 2017](../../Conference/NSDI-2017/)               | <ul><li>UC Berkeley</li></ul> | <ul><li><a href="../../Conference/NSDI-2017/clipper.md">Personal Notes</a></li><li><a href="https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/crankshaw">Paper</a></li><li><a href="https://github.com/ucbrise/clipper">Code</a></li></ul>                                                                                                                                                 | Caching, batching, adaptive model selection.          |
| **TensorFlow Serving** | NIPS 2017 Workshop on ML Systems                       | <ul><li>Google</li></ul>      | <ul><li><a href="https://arxiv.org/abs/1712.06139">Paper</a></li></ul>                                                                                                                                                                                                                                                                                                                                            |                                                       |

## Auto-Configuration System

|      Name     | Conference                                               | Institution                                                          | Links                                                                                                                                                                                                                                     | Remarks                                       |
| :-----------: | -------------------------------------------------------- | -------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------- |
|   **Falcon**  | [SoCC 2022](../../reading-notes/conference/socc-2022/)   | <ul><li>Institute of Software, Chinese Academy of Sciences</li></ul> | <ul><li><a href="../../reading-notes/conference/socc-2022/falcon.md">Personal Notes</a></li><li><a href="https://dl.acm.org/doi/10.1145/3542929.3563485">Paper</a></li><li><a href="https://github.com/dos-lab/Falcon">Code</a></li></ul> | Characterize a DL model by its key operators. |
| **Morphling** | [SoCC 2021](../../reading-notes/conference/socc-2021.md) | <ul><li>HKUST</li><li>Alibaba</li></ul>                              | <ul><li><a href="https://dl.acm.org/doi/10.1145/3472883.3486987">Paper</a></li><li><a href="https://github.com/kubedl-io/morphling">Code</a></li></ul>                                                                                    | Meta learning; bayesian optimization.         |

## Survey

| Survey                                                                                          |                     Conference                    | Institute                                                                                                                   | Links                                                                  |
| ----------------------------------------------------------------------------------------------- | :-----------------------------------------------: | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| A survey of multi-tenant deep learning inference on GPU                                         | MLSys 2022 Workshop on Cloud Intelligence / AIOps | <ul><li>George Mason University</li><li>Microsoft</li><li>University of Maryland</li></ul>                                  | <ul><li><a href="https://arxiv.org/abs/2203.09040">arXiv</a></li></ul> |
| A survey of large-scale deep learning serving system optimization: Challenges and opportunities |                  arXiv 2111.14247                 | <ul><li>George Mason University</li><li>Microsoft</li><li>University of Pittsburgh</li><li>University of Maryland</li></ul> | <ul><li><a href="https://arxiv.org/abs/2111.14247">arXiv</a></li></ul> |
