# Model Serving

## Inference Serving System

Large language models (LLMs) are _hot_ and _diverse_ compared to conventional models. Therefore, I have classified the related works for LLMs in [another paper list](llm.md).

<table data-full-width="true"><thead><tr><th width="154">Name</th><th width="163">Conference</th><th width="157">Institution</th><th width="161">Links</th><th>Remarks</th></tr></thead><tbody><tr><td><strong>REEF</strong></td><td><a href="../../reading-notes/conference/osdi-2022/">OSDI 2022</a></td><td><ul><li>SJTU</li></ul></td><td><ul><li><a href="../../reading-notes/conference/osdi-2022/reef.md">Personal Notes</a></li><li><a href="https://www.usenix.org/conference/osdi22/presentation/han">Paper</a></li><li><a href="https://github.com/SJTU-IPADS/reef">Code</a></li><li><a href="https://github.com/SJTU-IPADS/disb">Benchmark</a></li><li><a href="https://github.com/SJTU-IPADS/reef-artifacts/tree/osdi22-ae">Artifact</a></li></ul></td><td>GPU <em>kernel preemption</em>; <em>concurrent</em> DNN inferences.</td></tr><tr><td><strong>INFaaS</strong></td><td><a href="../../reading-notes/conference/atc-2021/">ATC 2021</a></td><td><ul><li>Stanford</li></ul></td><td><ul><li><a href="https://www.usenix.org/conference/atc21/presentation/romero">Paper</a></li><li><a href="https://github.com/stanford-mast/INFaaS">Code</a></li></ul></td><td><strong>Best Paper</strong>; consider <em>model-variants</em>.</td></tr><tr><td><strong>Clipper</strong></td><td><a href="../../reading-notes/conference/nsdi-2017/">NSDI 2017</a></td><td><ul><li>UC Berkeley</li></ul></td><td><ul><li><a href="../../reading-notes/conference/nsdi-2017/clipper.md">Personal Notes</a></li><li><a href="https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/crankshaw">Paper</a></li><li><a href="https://github.com/ucbrise/clipper">Code</a></li></ul></td><td>Caching, batching, adaptive model selection.</td></tr><tr><td><strong>TensorFlow Serving</strong></td><td>NIPS 2017 Workshop on ML Systems</td><td><ul><li>Google</li></ul></td><td><ul><li><a href="https://arxiv.org/abs/1712.06139">Paper</a></li></ul></td><td></td></tr></tbody></table>

## Auto-Configuration System

| Name          |                        Conference                        | Institution                                                          | Links                                                                                                                                                                                                                                     | Remarks                                       |
| ------------- | :------------------------------------------------------: | -------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------- |
| **Falcon**    |  [SoCC 2022](../../reading-notes/conference/socc-2022/)  | <ul><li>Institute of Software, Chinese Academy of Sciences</li></ul> | <ul><li><a href="../../reading-notes/conference/socc-2022/falcon.md">Personal Notes</a></li><li><a href="https://dl.acm.org/doi/10.1145/3542929.3563485">Paper</a></li><li><a href="https://github.com/dos-lab/Falcon">Code</a></li></ul> | Characterize a DL model by its key operators. |
| **Morphling** | [SoCC 2021](../../reading-notes/conference/socc-2021.md) | <ul><li>HKUST</li><li>Alibaba</li></ul>                              | <ul><li><a href="https://dl.acm.org/doi/10.1145/3472883.3486987">Paper</a></li><li><a href="https://github.com/kubedl-io/morphling">Code</a></li></ul>                                                                                    | Meta learning; bayesian optimization.         |

## Survey

<table><thead><tr><th width="238">Survey</th><th align="center">Conference</th><th>Institute</th><th>Links</th></tr></thead><tbody><tr><td><strong>A survey of multi-tenant deep learning inference on GPU</strong></td><td align="center">MLSys 2022 Workshop on Cloud Intelligence / AIOps</td><td><ul><li>George Mason University</li><li>Microsoft</li><li>University of Maryland</li></ul></td><td><ul><li><a href="https://arxiv.org/abs/2203.09040">arXiv</a></li></ul></td></tr><tr><td><strong>A survey of large-scale deep learning serving system optimization: Challenges and opportunities</strong></td><td align="center">arXiv 2111.14247</td><td><ul><li>George Mason University</li><li>Microsoft</li><li>University of Pittsburgh</li><li>University of Maryland</li></ul></td><td><ul><li><a href="https://arxiv.org/abs/2111.14247">arXiv</a></li></ul></td></tr></tbody></table>
