# Deep Learning Recommendation Model (DLRM)

## Optimizing Embedding Tables

<table><thead><tr><th width="139">Name</th><th width="142" align="center">Conference</th><th width="167">Institution</th><th width="127">Links</th><th>Remarks</th></tr></thead><tbody><tr><td><strong>AdaEmbed</strong></td><td align="center"><a href="../../reading-notes/conference/osdi-2023.md">OSDI 2023</a></td><td><ul><li>UMich</li><li>Meta</li></ul></td><td><ul><li><a href="https://www.usenix.org/conference/osdi23/presentation/lai">Paper</a></li></ul></td><td>In-training pruning</td></tr><tr><td><strong>EVStore</strong></td><td align="center"><a href="../../reading-notes/conference/asplos-2023/">ASPLOS 2023</a></td><td><ul><li>UChicago</li><li>Many others</li></ul></td><td><ul><li><a href="../../reading-notes/conference/asplos-2023/evstore.md">Personal Notes</a></li><li><a href="https://dl.acm.org/doi/10.1145/3575693.3575718">Paper</a></li><li><a href="https://github.com/ucare-uchicago/ev-store-dlrm">Code</a></li></ul></td><td>A <em>caching</em> layer optimized for embedding <em>access patterns</em>.</td></tr><tr><td><strong>DisaggRec</strong></td><td align="center">arXiv 2212.00939</td><td><ul><li>Meta AI</li><li>WashU</li><li>UPenn</li><li>Cornell</li><li>Intel</li></ul></td><td><ul><li><a href="../../reading-notes/miscellaneous/arxiv/2022/disaggrec-architecting-disaggregated-systems-for-large-scale-personalized-recommendation.md">Personal Notes</a></li><li><a href="https://arxiv.org/abs/2212.00939">Paper</a></li></ul></td><td><em>Disaggregated</em> system; <em>decouple</em> CPUs and memory resources; <em>partition embedding tables</em>.</td></tr></tbody></table>

## Deep Learning Recommendation Model

|   Name  | Conference   | Institution               | Links                                                                                                                                                                                                                    | Remarks                                                                                 |
| :-----: | ------------ | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------- |
| **ETA** | DLP-KDD 2022 | <ul><li>Alibaba</li></ul> | <ul><li><a href="https://arxiv.org/abs/2209.12212">Paper</a></li></ul>                                                                                                                                                   | _Efficient target attention_ network; locality-sensitive hashing; deployed on _Taobao_. |
| **WDL** | DLRS 2016    | <ul><li>Google</li></ul>  | <ul><li><a href="../../reading-notes/miscellaneous/arxiv/2016/wide-and-deep-learning-for-recommender-systems.md">Personal Notes</a></li><li><a href="https://dl.acm.org/doi/10.1145/2988450.2988454">Paper</a></li></ul> | Wide & Deep model.                                                                      |
