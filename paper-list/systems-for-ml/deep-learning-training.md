# Deep Learning Training Systems

## Parallelism

|         Name        | Conference                                                     | Institution              | Links                                                                         | Remarks                                                                                          |
| :-----------------: | -------------------------------------------------------------- | ------------------------ | ----------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |
|       **Tofu**      | [EuroSys 2019](../../reading-notes/conference/eurosys-2019.md) | <ul><li>NYU</li></ul>    | <ul><li><a href="https://doi.org/10.1145/3302424.3303953">Paper</a></li></ul> | _Automatic partition_ a dataflow graph of fine-grained tensor operations.                        |
| **CNN Parallelism** | arXiv 1404.599                                                 | <ul><li>Google</li></ul> | <ul><li><a href="https://arxiv.org/abs/1404.5997">Paper</a></li></ul>         | _Data parallelism_ for _convolutional layers_; _model parallelism_ for _fully-connected layers_. |

## Network Communication

|    Name    | Conference                               | Institution                                             | Links                                                                                                                                                                                                                                                                                                                                                | Remarks                                                                                               |
| :--------: | ---------------------------------------- | ------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| **BytePS** | [OSDI 2020](../../Conference/OSDI-2020/) | <ul><li>Tsinghua University</li><li>ByteDance</li></ul> | <ul><li><a href="../../reading-notes/conference/osdi-2020/a-unified-architecture-for-accelerating-distributed-dnn-training-in-heterogeneous-gpu-cpu-clusters.md">Personal Notes</a></li><li><a href="https://www.usenix.org/conference/osdi20/presentation/jiang">Paper</a></li><li><a href="https://github.com/bytedance/byteps">Code</a></li></ul> | _Communication framework; leverage_ spare CPU and bandwidth resources_;_ consider _network topology._ |

## Reduce GPU Memory Footprints

### GPU Sharing

|     Name    | Conference                                                 | Institution                                                                   | Links                                                                                                                                                                                           | Remarks                                                                              |
| :---------: | ---------------------------------------------------------- | ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |
|   **Zico**  | [ATC 2021](../../Conference/ATC-2021/)                     | <ul><li>UNIST</li><li>Ajou University</li><li>Alibaba</li><li>KAIST</li></ul> | <ul><li><a href="../../Conference/ATC-2021/zico.md">Personal Notes</a></li><li><a href="https://www.usenix.org/conference/atc21/presentation/lim">Paper</a></li></ul>                           | Reduce the _overall_ GPU consumption for _co-located_ DNN training jobs; NVIDIA MPS. |
|  **Salus**  | [MLSys 2020](../../reading-notes/conference/mlsys-2020.md) | <ul><li><a href="https://symbioticlab.org/">UMich SymbioticLab</a></li></ul>  | <ul><li><a href="https://proceedings.mlsys.org/paper/2020/file/f7177163c833dff4b38fc8d2872f1ec6-Paper.pdf">Paper</a></li><li><a href="https://github.com/symbioticlab/salus">Code</a></li></ul> | Fine-grained GPU sharing; _customized_ TensorFlow.                                   |
| **Gandiva** | [OSDI 2018](../../reading-notes/conference/osdi-2018/)     | <ul><li>MSRA</li></ul>                                                        | <ul><li><a href="https://www.usenix.org/conference/osdi18/presentation/xiao">Paper</a></li></ul>                                                                                                | Time slicing; suspend and resume; mini-batch granularity.                            |

### Tensor Swapping / Recomputation

|        Name       | Conference                                                   | Institution                                                                                                                                       | Links                                                                                                                                                                                                                                                         | Remarks                                                                                              |
| :---------------: | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |
|  **SwapAdvisor**  | [ASPLOS 2020](../../reading-notes/conference/asplos-2020.md) | <ul><li>NYU</li></ul>                                                                                                                             | <ul><li><a href="https://dl.acm.org/doi/10.1145/3373376.3378530">Paper</a></li></ul>                                                                                                                                                                          | _Tensor swapping_; consider both _GPU memory allocation_ and _operator scheduling_.                  |
|    **Capuchin**   | [ASPLOS 2020](../../reading-notes/conference/asplos-2020.md) | <ul><li>HUST</li><li>MSRA</li><li>University of Southern California</li></ul>                                                                     | <ul><li><a href="https://dl.acm.org/doi/10.1145/3373376.3378505">Paper</a></li></ul>                                                                                                                                                                          | Combination of _tensor swapping_ and _recomputation_.                                                |
|   **Checkmate**   | [MLSys 2020](../../reading-notes/conference/mlsys-2020.md)   | <ul><li>UC Berkeley</li></ul>                                                                                                                     | <ul><li><a href="https://proceedings.mlsys.org/paper/2020/hash/084b6fbb10729ed4da8c3d3f5a3ae7c9-Abstract.html">Paper</a></li><li><a href="https://github.com/parasj/checkmate">Code</a></li></ul>                                                             | Define _tensor recomputation_ as an optimization problem.                                            |
|  **Superneurons** | PPoPP 2018                                                   | <ul><li>Brown University</li><li>UESTC</li><li>Los Alamos National Laboratory</li><li>Pacific Northwest National Laboratory</li><li>MIT</li></ul> | <ul><li><a href="https://dl.acm.org/doi/10.1145/3200691.3178491">Paper</a></li></ul>                                                                                                                                                                          | _Cost-aware_ recomputation; _remove the convolutional layer tensor_ with low computational overhead. |
|      **vDNN**     | MICRO 2016                                                   | <ul><li>NVIDIA</li></ul>                                                                                                                          | <ul><li><a href="https://dl.acm.org/doi/10.5555/3195638.3195660">Paper</a></li></ul>                                                                                                                                                                          | Predictively _swap tensors_ to _overlap the CPU-GPU communication time_.                             |
| **Memory Monger** | arXiv 1604.06174                                             | <ul><li>UW</li><li>Dato. Inc</li><li>MIT</li></ul>                                                                                                | <ul><li><a href="../../Miscellaneous/arXiv-2016/training-deep-nets-with-sublinear-memory-cost.md">Personal Notes</a></li><li><a href="https://arxiv.org/abs/1604.06174">Paper</a></li><li><a href="https://github.com/dmlc/mxnet-memonger">Code</a></li></ul> | Sublinear memory cost; trade computation for memory.                                                 |

### Compression

|   Name   | Conference | Institution                                                                      | Links                                                                                                                      | Remarks            |
| :------: | ---------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- | ------------------ |
| **Echo** | ISCA 2020  | <ul><li>University of Toronto</li></ul>                                          | <ul><li><a href="https://dl.acm.org/doi/abs/10.1109/ISCA45697.2020.00092">Paper</a></li></ul>                              | LSTM RNN training. |
| **Gist** | ISCA 2018  | <ul><li>Microsoft Research</li><li>UMich</li><li>University of Toronto</li></ul> | <ul><li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2018/04/fiddle-gist-isca18.pdf">Paper</a></li></ul> | Data encoding.     |
