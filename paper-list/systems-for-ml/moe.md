# Mixture of Experts (MoE)

## MoE Training

* Accelerating Distributed MoE Training and Inference with Lina ([ATC 2023](../../reading-notes/conference/atc-2023/)) \[[Paper](https://www.usenix.org/conference/atc23/presentation/li-jiamin)]
  * CityU & ByteDance & CUHK
* SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization ([ATC 2023](../../reading-notes/conference/atc-2023/)) \[[Paper](https://www.usenix.org/conference/atc23/presentation/zhai)] \[[Code](https://github.com/thu-pacman/SmartMoE-AE)]
  * THU

## MoE Inference

* Accelerating Distributed MoE Training and Inference with Lina ([ATC 2023](../../reading-notes/conference/atc-2023/)) \[[Paper](https://www.usenix.org/conference/atc23/presentation/li-jiamin)]
  * CityU & ByteDance & CUHK
* Optimizing Dynamic Neural Networks with Brainstorm ([OSDI 2023](../../reading-notes/conference/osdi-2023.md)) \[[Paper](https://www.usenix.org/conference/osdi23/presentation/cui)]
  * SJTU & MSRA & USTC
