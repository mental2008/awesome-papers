# GPU Sharing

## GPU Temporal Sharing

<table><thead><tr><th width="145">Name</th><th width="128" align="center">Conf/Journal</th><th width="149">Institute</th><th>Links</th><th>Remarks</th></tr></thead><tbody><tr><td><strong>TGS</strong></td><td align="center"><a href="../../reading-notes/conference/nsdi-2023/">NSDI 2023</a></td><td><ul><li>PKU</li></ul></td><td><ul><li><a href="https://www.usenix.org/conference/nsdi23/presentation/wu">Paper</a></li><li><a href="https://github.com/pkusys/TGS">Code</a></li></ul></td><td><em>Transparent</em> GPU sharing; adaptive rate control; unified memory.</td></tr><tr><td>REEF</td><td align="center"><a href="../../reading-notes/conference/osdi-2022/">OSDI 2022</a></td><td><ul><li>SJTU</li></ul></td><td><ul><li><a href="../../reading-notes/conference/osdi-2022/reef.md">Personal Notes</a></li><li><a href="https://www.usenix.org/conference/osdi22/presentation/han">Paper</a></li><li><a href="https://github.com/SJTU-IPADS/reef">Code</a></li><li><a href="https://github.com/SJTU-IPADS/disb">Benchmark</a></li><li><a href="https://github.com/SJTU-IPADS/reef-artifacts/tree/osdi22-ae">Artifact</a></li></ul></td><td>GPU kernel preemption; dynamic kernel padding.</td></tr><tr><td><strong>MLaaS</strong></td><td align="center"><a href="../../reading-notes/conference/nsdi-2022.md">NSDI 2022</a></td><td><ul><li>HKUST</li><li>Alibaba</li></ul></td><td><ul><li><a href="https://www.usenix.org/conference/nsdi22/presentation/weng">Paper</a></li><li><a href="https://github.com/alibaba/clusterdata/tree/master/cluster-trace-gpu-v2020">Trace</a></li></ul></td><td>GPU sharing workloads; trace analysis.</td></tr><tr><td><strong>Gemini</strong></td><td align="center"><a href="../../reading-notes/journal/tcc/tcc-2021/">TCC 2021</a></td><td><ul><li>National Tsing Hua University</li></ul></td><td><ul><li><a href="https://ieeexplore.ieee.org/document/9566822">Paper</a></li><li><a href="https://github.com/NTHU-LSALAB/Gemini">Code</a></li></ul></td><td>Enable fine-grained GPU allocation; launch kernels together.</td></tr><tr><td><strong>AntMan</strong></td><td align="center"><a href="../../reading-notes/conference/osdi-2020/">OSDI 2020</a></td><td><ul><li>Alibaba</li></ul></td><td><ul><li><a href="https://www.usenix.org/conference/osdi20/presentation/xiao">Paper</a></li><li><a href="https://github.com/alibaba/GPU-scheduler-for-deep-learning">Code</a></li></ul></td><td>Enable GPU sharing in DL frameworks (TensorFlow/PyTorch); schedule operators.</td></tr><tr><td><strong>KubeShare</strong></td><td align="center"><a href="../../reading-notes/conference/hpdc-2020/">HPDC 2020</a></td><td><ul><li>National Tsing Hua University</li></ul></td><td><ul><li><a href="../../reading-notes/conference/hpdc-2020/kubeshare.md">Personal Notes</a></li><li><a href="https://dl.acm.org/doi/10.1145/3369583.3392679">Paper</a></li><li><a href="https://github.com/NTHU-LSALAB/KubeShare">Code</a></li></ul></td><td>Kubernetes; CUDA API remoting.</td></tr><tr><td><strong>GaiaGPU</strong></td><td align="center"><a href="../../reading-notes/conference/ispa-iucc-bdcloud-socialcom-sustaincom-2018/">ISPA/IUCC/BDCloud/SocialCom/SustainCom 2018</a></td><td><ul><li>Tencent</li><li>PKU</li></ul></td><td><ul><li><a href="../../reading-notes/conference/ispa-iucc-bdcloud-socialcom-sustaincom-2018/gaiagpu.md">Personal Notes</a></li><li><a href="https://ieeexplore.ieee.org/document/8672318">Paper</a></li><li><a href="https://github.com/tkestack/gpu-manager">Code</a></li></ul></td><td>Kubernetes; CUDA API remoting.</td></tr></tbody></table>

## GPU Spatial Sharing

* [NVIDIA Multi-Instance GPU (MIG)](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)
  * Partition the GPU into as many as _seven instances_, each _fully isolated_ with its own high-bandwidth memory, cache, and compute cores.
  * Available for NVIDIA H100, A100, and A30 GPUs.
* [NVIDIA Multi-Process Service (MPS)](https://docs.nvidia.com/deploy/mps/index.html)
  * Transparently enable co-operative multi-process CUDA applications.
  * Terminating an MPS client without synchronizing with all outstanding GPU work (via Ctrl-C / program exception such as segfault / signals, etc.) can leave the MPS server and other MPS clients in an undefined state, which may result in hangs, unexpected failures, or corruptions.
* [NVIDIA CUDA Multi-Stream](https://docs.nvidia.com/cuda/cuda-runtime-api/group\_\_CUDART\_\_STREAM.html)
  * Stream: a sequence of operations that execute in issue-order on the GPU.
  * Perform multiple CUDA operations _simultaneously_.

## Survey

| Survey                                                                |                          Journal                         | Institute                                    | Links                                                                                                                                                                                     |
| --------------------------------------------------------------------- | :------------------------------------------------------: | -------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **GPU virtualization and scheduling methods: A comprehensive survey** | [CSUR 2017](../../reading-notes/journal/csur/csur-2017/) | <ul><li>Queenâ€™s University Belfast</li></ul> | <ul><li><a href="../../reading-notes/journal/csur/csur-2017/gpu-virtualization-survey.md">Personal Notes</a></li><li><a href="https://dl.acm.org/doi/10.1145/3068281">Paper</a></li></ul> |
