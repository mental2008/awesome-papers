# GPU Sharing

## GPU Temporal Sharing

| Name          |                                                 Conf/Journal                                                 | Institute                                       | Links                                                                                                                                                                                                                                                              | Remarks                                                                       |
| ------------- | :----------------------------------------------------------------------------------------------------------: | ----------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------- |
| **MLaaS**     |                           [NSDI 2022](../../reading-notes/conference/nsdi-2022.md)                           | <ul><li>HKUST</li><li>Alibaba</li></ul>         | <ul><li><a href="https://www.usenix.org/conference/nsdi22/presentation/weng">Paper</a></li><li><a href="https://github.com/alibaba/clusterdata/tree/master/cluster-trace-gpu-v2020">Trace</a></li></ul>                                                            | GPU sharing workloads; trace analysis.                                        |
| **Gemini**    |            [TCC 2021](../../reading-notes/journal/ieee-transactions-on-cloud-computing/tcc-2021/)            | <ul><li>National Tsing Hua University</li></ul> | <ul><li><a href="https://ieeexplore.ieee.org/document/9566822">Paper</a></li><li><a href="https://github.com/NTHU-LSALAB/Gemini">Code</a></li></ul>                                                                                                                | Enable fine-grained GPU allocation; launch kernels together.                  |
| **AntMan**    |                                   [OSDI 2020](../../Conference/OSDI-2020/)                                   | <ul><li>Alibaba</li></ul>                       | <ul><li><a href="https://www.usenix.org/conference/osdi20/presentation/xiao">Paper</a></li><li><a href="https://github.com/alibaba/GPU-scheduler-for-deep-learning">Code</a></li></ul>                                                                             | Enable GPU sharing in DL frameworks (TensorFlow/PyTorch); schedule operators. |
| **KubeShare** |                                   [HPDC 2020](../../Conference/HPDC-2020/)                                   | <ul><li>National Tsing Hua University</li></ul> | <ul><li><a href="../../Conference/HPDC-2020/kubeshare.md">Personal Notes</a></li><li><a href="https://dl.acm.org/doi/10.1145/3369583.3392679">Paper</a></li><li><a href="https://github.com/NTHU-LSALAB/KubeShare">Code</a></li></ul>                              | Kubernetes; CUDA API remoting.                                                |
| **GaiaGPU**   | [ISPA/IUCC/BDCloud/SocialCom/SustainCom 2018](../../Conference/ISPA-IUCC-BDCloud-SocialCom-SustainCom-2018/) | <ul><li>Tencent</li><li>PKU</li></ul>           | <ul><li><a href="../../Conference/ISPA-IUCC-BDCloud-SocialCom-SustainCom-2018/gaiagpu.md">Personal Notes</a></li><li><a href="https://ieeexplore.ieee.org/document/8672318">Paper</a></li><li><a href="https://github.com/tkestack/gpu-manager">Code</a></li></ul> | Kubernetes; CUDA API remoting.                                                |

## GPU Spatial Sharing

* [NVIDIA Multi-Instance GPU (MIG)](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)
  * Partition the GPU into as many as _seven instances_, each _fully isolated_ with its own high-bandwidth memory, cache, and compute cores.
  * Available for NVIDIA H100, A100, and A30 GPUs.
* [NVIDIA Multi-Process Service (MPS)](https://docs.nvidia.com/deploy/mps/index.html)
  * Transparently enable co-operative multi-process CUDA applications.
  * Terminating an MPS client without synchronizing with all outstanding GPU work (via Ctrl-C / program exception such as segfault / signals, etc.) can leave the MPS server and other MPS clients in an undefined state, which may result in hangs, unexpected failures, or corruptions.
* [NVIDIA CUDA Multi-Stream](https://docs.nvidia.com/cuda/cuda-runtime-api/group\_\_CUDART\_\_STREAM.html)
  * Stream: a sequence of operations that execute in issue-order on the GPU.
  * Perform multiple CUDA operations _simultaneously_.

## Survey

| Survey                                                            | Journal                                                                   | Institute                                    | Links                                                                                                                                                                                                      |
| ----------------------------------------------------------------- | ------------------------------------------------------------------------- | -------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| GPU virtualization and scheduling methods: A comprehensive survey | [CSUR 2017](../../reading-notes/journal/acm-computing-surveys/csur-2017/) | <ul><li>Queenâ€™s University Belfast</li></ul> | <ul><li><a href="../../reading-notes/journal/acm-computing-surveys/csur-2017/gpu-virtualization-survey.md">Personal Notes</a></li><li><a href="https://dl.acm.org/doi/10.1145/3068281">Paper</a></li></ul> |
