# ATC 2022

## Metadata

2022 USENIX Annual Technical Conference
- Homepage: [https://www.usenix.org/conference/atc22](https://www.usenix.org/conference/atc22)
- Paper lists: [https://www.usenix.org/conference/atc22/technical-sessions](https://www.usenix.org/conference/atc22/technical-sessions)

## Papers that appeal to me

### DNN inference

- DNN batching inference system to reduce the latency and improve the throughput: [DVABatch, SJTU](https://www.usenix.org/conference/atc22/presentation/cui) [ [Artifact](https://github.com/sjtu-epcc/DVABatch) ]
- DNN inference scheduling framework to improve the GPU utilization under SLO constraints: [Gpulet, KAIST](https://www.usenix.org/conference/atc22/presentation/choi-seungbeom) [ [Code](https://github.com/casys-kaist/glet) ]
- Secure DNN inference system to ensure model confidentiality, low latency, high accuracy with integrity protection: [SOTER, HKU](https://www.usenix.org/conference/atc22/presentation/shen) [ [Artifact](https://github.com/hku-systems/SOTER) ]

### DNN training

- Distributed training framework for large models: [Whale, Alibaba](https://www.usenix.org/conference/atc22/presentation/jia-xianyan) [ [Code](https://github.com/alibaba/EasyParallelLibrary) ]

### Resource manager

- Resource manager which co-locates cloud gaming and DL training to improve the GPU utilization: [PilotFish, SJTU & MSRA](https://www.usenix.org/conference/atc22/presentation/zhang-wei)
- GPU memory manager which harvests the temporarily available neighbor GPUs' memory: [memHarvester, KAIST](https://www.usenix.org/conference/atc22/presentation/choi-sangjin)

